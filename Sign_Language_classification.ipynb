{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Sign Language classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1m5fUUufv_-roXcK-R790FndLGsET8qiL",
      "authorship_tag": "ABX9TyO4x7e2LheepeSzcTFkvsYM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lilyaYAHIAOUI/Sign-Language-Classification/blob/main/Sign_Language_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvjqPN6F7GA4"
      },
      "source": [
        "import urllib.request\n",
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import tensorflow as tf\n",
        "from shutil import copyfile\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FuYULLw7OFC"
      },
      "source": [
        "# Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKxWNpym7JWW"
      },
      "source": [
        "IMAGE_DIR = \"/content/drive/MyDrive/competition/Zindi/Sign Language classification/data/Images\"\n",
        "IMAGE_TEST_DIR = \"/content/drive/MyDrive/competition/Zindi/Sign Language classification/data/Test\"\n",
        "IMAGE_TRAIN_DIR = \"/content/drive/MyDrive/competition/Zindi/Sign Language classification/data/final_data\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrnkNDxs7QVB"
      },
      "source": [
        "df_train = pd.read_csv(\"/content/drive/MyDrive/competition/Zindi/Sign Language classification/data/Train.csv\")\n",
        "df_test = pd.read_csv(\"/content/drive/MyDrive/competition/Zindi/Sign Language classification/data/Test.csv\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpLnAdU47ScN",
        "outputId": "2369d498-1cec-4d5c-a44d-a0b0cf3b46c9"
      },
      "source": [
        "print(\"Number of training images\", df_train.shape[0])\n",
        "print(\"Number of testing images\", df_test.shape[0])\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training images 6249\n",
            "Number of testing images 2679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ih1F-yvB7YZZ",
        "outputId": "acb725f2-7395-40cb-9b23-50fae4170b01"
      },
      "source": [
        "df_train.columns"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['img_IDS', 'Label'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p237E4bb7Z5H",
        "outputId": "7beb97fd-86c0-49df-cb78-2478aa3b260c"
      },
      "source": [
        "df_train[\"Label\"].unique()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Temple', 'Church', 'Enough/Satisfied', 'Me', 'Love', 'Mosque',\n",
              "       'You', 'Friend', 'Seat'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZnIh10B7cLX",
        "outputId": "39d46a6c-3258-466e-c3f8-a8a396b99bee"
      },
      "source": [
        "print(df_train[\"Label\"].nunique())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lc1rbkUu7ifA"
      },
      "source": [
        "# Process Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HirAxLGp7dxU",
        "outputId": "4aa2d95e-2dbe-43f6-cf3b-d8cf89802dc6"
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      brightness_range=[0.2,1.0],\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest',\n",
        "      validation_split=0.2 )\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(IMAGE_TRAIN_DIR,\n",
        "                                                    batch_size=30,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    target_size=(224,224),\n",
        "                                                    shuffle=True)\n",
        "\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(IMAGE_TRAIN_DIR,\n",
        "                                                    batch_size=30,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    target_size=(224,224),\n",
        "                                                    shuffle=True,\n",
        "                                                    subset=\"validation\",\n",
        "                                                    )\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6255 images belonging to 9 classes.\n",
            "Found 1246 images belonging to 9 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABceSWVn7o04"
      },
      "source": [
        "def preprocess_image_input(input_images):\n",
        "  input_images = input_images.astype('float32')\n",
        "  output_ims = tf.keras.applications.resnet50.preprocess_input(input_images)\n",
        "  return output_ims\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m9suUGk7zyO",
        "outputId": "c2bdb496-42d8-47bb-e072-1ba7a514a34a"
      },
      "source": [
        "'''\n",
        "Feature Extraction is performed by ResNet50 pretrained on imagenet weights. \n",
        "Input size is 224 x 224.\n",
        "'''\n",
        "def feature_extractor(inputs):\n",
        "\n",
        "  feature_extractor = tf.keras.applications.resnet.ResNet50(input_shape=(224, 224, 3),\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')(inputs)\n",
        "  return feature_extractor\n",
        "\n",
        "\n",
        "'''\n",
        "Defines final dense layers and subsequent softmax layer for classification.\n",
        "'''\n",
        "def classifier(inputs):\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(inputs)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(1024, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dense(9, activation=\"softmax\", name=\"classification\")(x)\n",
        "    return x\n",
        "\n",
        "'''\n",
        "Since input image size is (32 x 32), first upsample the image by factor of (7x7) to transform it to (224 x 224)\n",
        "Connect the feature extraction and \"classifier\" layers to build the model.\n",
        "'''\n",
        "def final_model(inputs):\n",
        "\n",
        "\n",
        "    resnet_feature_extractor = feature_extractor(inputs)\n",
        "    classification_output = classifier(resnet_feature_extractor)\n",
        "\n",
        "    return classification_output\n",
        "\n",
        "\n",
        "def define_compile_model():\n",
        "  inputs = tf.keras.layers.Input(shape=(224,224,3))\n",
        "  \n",
        "  classification_output = final_model(inputs) \n",
        "  model = tf.keras.Model(inputs=inputs, outputs = classification_output)\n",
        " \n",
        "  # compile the model\n",
        "  model.compile(optimizer=RMSprop(lr=0.0001),\n",
        "              loss= tf.keras.losses.CategoricalCrossentropy() ,\n",
        "              metrics=['acc'])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "model = define_compile_model()\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n",
            "94781440/94765736 [==============================] - 1s 0us/step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 2048)             0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              2098176   \n",
            "                                                                 \n",
            " classification (Dense)      (None, 9)                 9225      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,695,113\n",
            "Trainable params: 25,641,993\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8J92AV47289",
        "outputId": "58072248-a1f1-430a-8fab-e3fe06b53a5a"
      },
      "source": [
        "# this will take around 20 minutes to complete\n",
        "EPOCHS = 10\n",
        "checkpoint = ModelCheckpoint(\"/content/drive/MyDrive/competition/Zindi/Sign Language classification/data/model_weights.h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint] \n",
        "\n",
        "history = model.fit(train_generator,validation_data =validation_generator, epochs=EPOCHS, callbacks=callbacks_list)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "209/209 [==============================] - ETA: 0s - loss: 0.8344 - acc: 0.7207\n",
            "Epoch 00001: val_loss improved from inf to 3.49157, saving model to /content/drive/MyDrive/competition/Zindi/Sign Language classification/data/model_weights.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r209/209 [==============================] - 1646s 8s/step - loss: 0.8344 - acc: 0.7207 - val_loss: 3.4916 - val_acc: 0.1108\n",
            "Epoch 2/10\n",
            "209/209 [==============================] - ETA: 0s - loss: 0.4491 - acc: 0.8603\n",
            "Epoch 00002: val_loss improved from 3.49157 to 2.45826, saving model to /content/drive/MyDrive/competition/Zindi/Sign Language classification/data/model_weights.h5\n",
            "209/209 [==============================] - 276s 1s/step - loss: 0.4491 - acc: 0.8603 - val_loss: 2.4583 - val_acc: 0.1284\n",
            "Epoch 3/10\n",
            "209/209 [==============================] - ETA: 0s - loss: 0.3708 - acc: 0.8870\n",
            "Epoch 00003: val_loss did not improve from 2.45826\n",
            "209/209 [==============================] - 277s 1s/step - loss: 0.3708 - acc: 0.8870 - val_loss: 2.9373 - val_acc: 0.2215\n",
            "Epoch 4/10\n",
            "209/209 [==============================] - ETA: 0s - loss: 0.3195 - acc: 0.9002\n",
            "Epoch 00004: val_loss did not improve from 2.45826\n",
            "209/209 [==============================] - 276s 1s/step - loss: 0.3195 - acc: 0.9002 - val_loss: 3.2610 - val_acc: 0.5241\n",
            "Epoch 5/10\n",
            "209/209 [==============================] - ETA: 0s - loss: 0.2785 - acc: 0.9173\n",
            "Epoch 00005: val_loss improved from 2.45826 to 0.38259, saving model to /content/drive/MyDrive/competition/Zindi/Sign Language classification/data/model_weights.h5\n",
            "209/209 [==============================] - 278s 1s/step - loss: 0.2785 - acc: 0.9173 - val_loss: 0.3826 - val_acc: 0.9109\n",
            "Epoch 6/10\n",
            "209/209 [==============================] - ETA: 0s - loss: 0.2448 - acc: 0.9221\n",
            "Epoch 00006: val_loss did not improve from 0.38259\n",
            "209/209 [==============================] - 280s 1s/step - loss: 0.2448 - acc: 0.9221 - val_loss: 0.9088 - val_acc: 0.8587\n",
            "Epoch 7/10\n",
            "209/209 [==============================] - ETA: 0s - loss: 0.2189 - acc: 0.9330\n",
            "Epoch 00007: val_loss did not improve from 0.38259\n",
            "209/209 [==============================] - 277s 1s/step - loss: 0.2189 - acc: 0.9330 - val_loss: 1.7479 - val_acc: 0.7961\n",
            "Epoch 8/10\n",
            "209/209 [==============================] - ETA: 0s - loss: 0.2143 - acc: 0.9348\n",
            "Epoch 00008: val_loss did not improve from 0.38259\n",
            "209/209 [==============================] - 276s 1s/step - loss: 0.2143 - acc: 0.9348 - val_loss: 1.5650 - val_acc: 0.8090\n",
            "Epoch 9/10\n",
            "209/209 [==============================] - ETA: 0s - loss: 0.1880 - acc: 0.9372\n",
            "Epoch 00009: val_loss improved from 0.38259 to 0.32619, saving model to /content/drive/MyDrive/competition/Zindi/Sign Language classification/data/model_weights.h5\n",
            "209/209 [==============================] - 279s 1s/step - loss: 0.1880 - acc: 0.9372 - val_loss: 0.3262 - val_acc: 0.9310\n",
            "Epoch 10/10\n",
            "209/209 [==============================] - ETA: 0s - loss: 0.1732 - acc: 0.9456\n",
            "Epoch 00010: val_loss did not improve from 0.32619\n",
            "209/209 [==============================] - 281s 1s/step - loss: 0.1732 - acc: 0.9456 - val_loss: 0.4160 - val_acc: 0.9222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0VNwEJIPCky"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xj7y_3Mz9ovA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fc60d0c-735a-4141-e40c-2f0ff13d32d1"
      },
      "source": [
        "test_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
        "test_generator =  test_datagen.flow_from_directory( IMAGE_TEST_DIR,\n",
        "                                                    shuffle=False,\n",
        "                                                          batch_size  = 20,\n",
        "                                                          class_mode  = None, \n",
        "                                                          target_size = (224, 224))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2679 images belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO6roPhhCOdL"
      },
      "source": [
        "test_generator.reset()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p21Y26o3CQOk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46892c54-74e9-4e4c-c502-2f33677741dd"
      },
      "source": [
        "pred = model.predict_generator(test_generator)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWQN5P7vCR0Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "372864d3-5ebc-43d2-d39c-926b2c017c4c"
      },
      "source": [
        "print(pred.shape)\n",
        "filenames=test_generator.filenames\n",
        "print(len(filenames))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2679, 9)\n",
            "2679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2N6vT9MzL1Co"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMrTNHbYCTGj"
      },
      "source": [
        "res1 = list(map(lambda st: str.replace(st, \"test/\", \"\"), filenames))\n",
        "res2 = list(map(lambda st: str.replace(st, \".jpg\", \"\"), res1))\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CgjpZ3QCUlU"
      },
      "source": [
        "actual = train_generator.classes\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLcI6ToHMsY6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d70d1abd-5068-4ea9-d759-b889d7ad85ac"
      },
      "source": [
        "type(pred)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tat9HembMuBb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38103e3f-bc67-4a22-b208-e4cd17fbe411"
      },
      "source": [
        "print(pred.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2679, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rzec0KWCVz-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "outputId": "3c22c2d2-fdc6-443c-c8bb-8f5664816894"
      },
      "source": [
        "import os\n",
        "submission = pd.DataFrame()\n",
        "submission[\"ID\"] = res2\n",
        "classes = [\"Church\",\"Enough/Satisfied\",\"Friend\",\"Love\",\"Me\",\"Mosque\",\"Seat\",\"Temple\",\"You\"]\n",
        "for i, c in enumerate(classes):\n",
        "  print(c)\n",
        "  print(i)\n",
        "  submission[c] = pred[:,i]\n",
        "submission.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Church\n",
            "0\n",
            "Enough/Satisfied\n",
            "1\n",
            "Friend\n",
            "2\n",
            "Love\n",
            "3\n",
            "Me\n",
            "4\n",
            "Mosque\n",
            "5\n",
            "Seat\n",
            "6\n",
            "Temple\n",
            "7\n",
            "You\n",
            "8\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Church</th>\n",
              "      <th>Enough/Satisfied</th>\n",
              "      <th>Friend</th>\n",
              "      <th>Love</th>\n",
              "      <th>Me</th>\n",
              "      <th>Mosque</th>\n",
              "      <th>Seat</th>\n",
              "      <th>Temple</th>\n",
              "      <th>You</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ImageID_00AVE728</td>\n",
              "      <td>1.184770e-13</td>\n",
              "      <td>1.736129e-17</td>\n",
              "      <td>4.802207e-17</td>\n",
              "      <td>1.421103e-21</td>\n",
              "      <td>2.224133e-20</td>\n",
              "      <td>2.472392e-22</td>\n",
              "      <td>1.529363e-21</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>2.524177e-22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ImageID_00CB7YJ2</td>\n",
              "      <td>5.568958e-17</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.236309e-17</td>\n",
              "      <td>1.859063e-20</td>\n",
              "      <td>9.652564e-22</td>\n",
              "      <td>1.281013e-21</td>\n",
              "      <td>5.642227e-21</td>\n",
              "      <td>8.569598e-21</td>\n",
              "      <td>1.425388e-20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ImageID_00HEGX6X</td>\n",
              "      <td>2.731526e-05</td>\n",
              "      <td>7.020454e-06</td>\n",
              "      <td>1.050166e-02</td>\n",
              "      <td>9.894543e-01</td>\n",
              "      <td>5.152945e-06</td>\n",
              "      <td>6.879222e-07</td>\n",
              "      <td>2.434829e-08</td>\n",
              "      <td>9.030932e-09</td>\n",
              "      <td>3.837778e-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ImageID_016X4GBI</td>\n",
              "      <td>1.082680e-07</td>\n",
              "      <td>9.304893e-09</td>\n",
              "      <td>2.456886e-12</td>\n",
              "      <td>6.517807e-12</td>\n",
              "      <td>4.562535e-13</td>\n",
              "      <td>9.999999e-01</td>\n",
              "      <td>2.283030e-13</td>\n",
              "      <td>4.555290e-14</td>\n",
              "      <td>3.810057e-12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ImageID_01ITRYRU</td>\n",
              "      <td>1.410902e-10</td>\n",
              "      <td>1.240785e-06</td>\n",
              "      <td>1.136330e-06</td>\n",
              "      <td>5.601510e-07</td>\n",
              "      <td>1.025994e-10</td>\n",
              "      <td>5.585187e-07</td>\n",
              "      <td>9.999963e-01</td>\n",
              "      <td>4.245483e-09</td>\n",
              "      <td>9.841157e-08</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 ID        Church  ...        Temple           You\n",
              "0  ImageID_00AVE728  1.184770e-13  ...  1.000000e+00  2.524177e-22\n",
              "1  ImageID_00CB7YJ2  5.568958e-17  ...  8.569598e-21  1.425388e-20\n",
              "2  ImageID_00HEGX6X  2.731526e-05  ...  9.030932e-09  3.837778e-06\n",
              "3  ImageID_016X4GBI  1.082680e-07  ...  4.555290e-14  3.810057e-12\n",
              "4  ImageID_01ITRYRU  1.410902e-10  ...  4.245483e-09  9.841157e-08\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vqucz_uScpM"
      },
      "source": [
        "submission.to_csv('/content/drive/MyDrive/competition/Zindi/Sign Language classification/data/sub3.csv',index=False)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvy9q4HqhEEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aed9816-da23-44a5-db0b-fa179c50f969"
      },
      "source": [
        "submission.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2679, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxC3YkSGiqma",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "8344d16f-bcfe-4797-aa7b-18ffd9a17ff4"
      },
      "source": [
        "submission[submission[\"ID\"]==\"ImageID_J3BM90F2\"]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Church</th>\n",
              "      <th>Enough/Satisfied</th>\n",
              "      <th>Friend</th>\n",
              "      <th>Love</th>\n",
              "      <th>Me</th>\n",
              "      <th>Mosque</th>\n",
              "      <th>Seat</th>\n",
              "      <th>Temple</th>\n",
              "      <th>You</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1420</th>\n",
              "      <td>ImageID_J3BM90F2</td>\n",
              "      <td>2.614250e-12</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.163008e-12</td>\n",
              "      <td>4.476683e-15</td>\n",
              "      <td>2.561795e-15</td>\n",
              "      <td>1.371805e-15</td>\n",
              "      <td>3.146961e-10</td>\n",
              "      <td>9.552415e-15</td>\n",
              "      <td>3.391745e-14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    ID        Church  ...        Temple           You\n",
              "1420  ImageID_J3BM90F2  2.614250e-12  ...  9.552415e-15  3.391745e-14\n",
              "\n",
              "[1 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5mWcr2wjyqz"
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": []
    }
  ]
}